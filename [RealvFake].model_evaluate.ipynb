{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9eb45a9-0583-4c6f-a086-06faa7e8f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56033d02-8168-4d48-883b-a1db42534c4f",
   "metadata": {},
   "source": [
    "Evaluation of Machine Learning Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a3d9f-6707-4156-bc29-ec1dd3e18290",
   "metadata": {},
   "source": [
    "Area under ROC curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "943920b3-07f7-4e25-b3ab-d3aff8d4681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Word2Vec:  0.9892886758693183\n",
      "GBT, Word2Vec:  0.9578940242388466\n",
      "LR, HashingTF:  0.9822348545142507\n",
      "GBT, HashingTF:  0.9657035070853843\n"
     ]
    }
   ],
   "source": [
    "prediction_lr_word2vec = model_lr_word2vec.transform(DF_test)\n",
    "prediction_gbt_word2vec = model_gbt_word2vec.transform(DF_test)\n",
    "prediction_lr_hashtf = model_lr_hashtf.transform(DF_test)\n",
    "prediction_gbt_hashtf = model_gbt_hashtf.transform(DF_test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Truthness')\n",
    "\n",
    "print('LR, Word2Vec: ',evaluator.evaluate(prediction_lr_word2vec))\n",
    "print('GBT, Word2Vec: ',evaluator.evaluate(prediction_gbt_word2vec))\n",
    "print('LR, HashingTF: ',evaluator.evaluate(prediction_lr_hashtf))\n",
    "print('GBT, HashingTF: ',evaluator.evaluate(prediction_gbt_hashtf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c83b89-4858-400d-93eb-b77ab8c44393",
   "metadata": {},
   "source": [
    "Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "271a70f3-3f00-4c01-9b21-42b22ea7344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\n",
      "LR, Word2Vec:  0.9510019819423035\n",
      "GBT, Word2Vec:  0.8816340013212949\n",
      "LR, HashingTF:  0.9241356529398811\n",
      "GBT, HashingTF:  0.9003523452983925\n"
     ]
    }
   ],
   "source": [
    "accuracy_lr_word2vec = prediction_lr_word2vec.filter(prediction_lr_word2vec.Truthness == prediction_lr_word2vec.prediction).count() / float(prediction_lr_word2vec.count())\n",
    "accuracy_gbt_word2vec = prediction_gbt_word2vec.filter(prediction_gbt_word2vec.Truthness == prediction_gbt_word2vec.prediction).count() / float(prediction_gbt_word2vec.count())\n",
    "accuracy_lr_hashtf = prediction_lr_hashtf.filter(prediction_lr_hashtf.Truthness == prediction_lr_hashtf.prediction).count() / float(prediction_lr_hashtf.count())\n",
    "accuracy_gbt_hashtf = prediction_gbt_hashtf.filter(prediction_gbt_hashtf.Truthness == prediction_gbt_hashtf.prediction).count() / float(prediction_gbt_hashtf.count())\n",
    "\n",
    "print('Accuracy:\\n')\n",
    "print('LR, Word2Vec: ', accuracy_lr_word2vec)\n",
    "print('GBT, Word2Vec: ', accuracy_gbt_word2vec)\n",
    "print('LR, HashingTF: ', accuracy_lr_hashtf)\n",
    "print('GBT, HashingTF: ', accuracy_gbt_hashtf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f92532-dfd1-4e73-b276-5e337d073696",
   "metadata": {},
   "source": [
    "Accuracy on TestSet.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bf9422b-db16-4148-9d39-93672c94191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                text|Truthness|\n",
      "+--------------------+---------+\n",
      "|Ime Udoka, head c...|        1|\n",
      "|The Artemis I roc...|        1|\n",
      "|Stefanos Tsitsipa...|        1|\n",
      "|Arguing that the ...|        0|\n",
      "|Addressing a rall...|        0|\n",
      "|Citing a lack of ...|        0|\n",
      "|California Gov. G...|        1|\n",
      "|Pete Antonacci, t...|        1|\n",
      "|An Arizona Superi...|        1|\n",
      "|The Department of...|        1|\n",
      "|Wyoming GOP Rep. ...|        1|\n",
      "|Rihanna will head...|        1|\n",
      "|A twelve-year-old...|        1|\n",
      "|Clicking back and...|        0|\n",
      "|Expressing shock ...|        0|\n",
      "|Saying those unab...|        0|\n",
      "|Assuring the nati...|        0|\n",
      "|Muttering in frus...|        0|\n",
      "|Wondering if anyo...|        0|\n",
      "|Having not yet co...|        0|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TestSet = pd.read_csv('TestSet.csv')\n",
    "TestSetSpark = sc.createDataFrame(TestSet)\n",
    "TestSetSpark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e553421-76a1-4a49-a494-62619561a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun entire pipelines for ML model testing on TestSet.csv\n",
    "testsetpipeline1 = Pipeline(stages=[textTokenizer,stopwords,text_w2v,text_norm])\n",
    "testsetpipeline2 = Pipeline(stages=[textTokenizer,stopwords,text_hash])\n",
    "\n",
    "TestSetSpark_mod1 = testsetpipeline1.fit(TestSetSpark).transform(TestSetSpark)\n",
    "TestSetSpark_mod2 = testsetpipeline2.fit(TestSetSpark).transform(TestSetSpark)\n",
    "\n",
    "prediction2_lr_word2vec = model_lr_word2vec.transform(TestSetSpark_mod1)\n",
    "prediction2_gbt_word2vec = model_gbt_word2vec.transform(TestSetSpark_mod1)\n",
    "prediction2_lr_hashtf = model_lr_hashtf.transform(TestSetSpark_mod2)\n",
    "prediction2_gbt_hashtf = model_gbt_hashtf.transform(TestSetSpark_mod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c33c7d5e-7259-4136-a169-c785b002844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "\n",
      "LR, Word2Vec:  0.5\n",
      "GBT, Word2Vec:  0.5\n",
      "LR, HashingTF:  0.6\n",
      "GBT, HashingTF:  0.45\n"
     ]
    }
   ],
   "source": [
    "accuracy2_lr_word2vec = prediction2_lr_word2vec.filter(prediction2_lr_word2vec.Truthness == prediction2_lr_word2vec.prediction).count() / float(prediction2_lr_word2vec.count())\n",
    "accuracy2_gbt_word2vec = prediction2_gbt_word2vec.filter(prediction2_gbt_word2vec.Truthness == prediction2_gbt_word2vec.prediction).count() / float(prediction2_gbt_word2vec.count())\n",
    "accuracy2_lr_hashtf = prediction2_lr_hashtf.filter(prediction2_lr_hashtf.Truthness == prediction2_lr_hashtf.prediction).count() / float(prediction2_lr_hashtf.count())\n",
    "accuracy2_gbt_hashtf = prediction2_gbt_hashtf.filter(prediction2_gbt_hashtf.Truthness == prediction2_gbt_hashtf.prediction).count() / float(prediction2_gbt_hashtf.count())\n",
    "\n",
    "print('Accuracy:\\n')\n",
    "print('LR, Word2Vec: ', accuracy2_lr_word2vec)\n",
    "print('GBT, Word2Vec: ', accuracy2_gbt_word2vec)\n",
    "print('LR, HashingTF: ', accuracy2_lr_hashtf)\n",
    "print('GBT, HashingTF: ', accuracy2_gbt_hashtf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb71068-5172-4870-9f9e-21576f27e8d2",
   "metadata": {},
   "source": [
    "So this is definitely not good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80d84d44-2f86-4627-af66-f570b7ece66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+\n",
      "|                text|Truthness|prediction|\n",
      "+--------------------+---------+----------+\n",
      "|Ime Udoka, head c...|        1|       1.0|\n",
      "|The Artemis I roc...|        1|       1.0|\n",
      "|Stefanos Tsitsipa...|        1|       1.0|\n",
      "|Arguing that the ...|        0|       1.0|\n",
      "|Addressing a rall...|        0|       1.0|\n",
      "|Citing a lack of ...|        0|       1.0|\n",
      "|California Gov. G...|        1|       1.0|\n",
      "|Pete Antonacci, t...|        1|       1.0|\n",
      "|An Arizona Superi...|        1|       1.0|\n",
      "|The Department of...|        1|       1.0|\n",
      "|Wyoming GOP Rep. ...|        1|       1.0|\n",
      "|Rihanna will head...|        1|       1.0|\n",
      "|A twelve-year-old...|        1|       1.0|\n",
      "|Clicking back and...|        0|       1.0|\n",
      "|Expressing shock ...|        0|       1.0|\n",
      "|Saying those unab...|        0|       1.0|\n",
      "|Assuring the nati...|        0|       1.0|\n",
      "|Muttering in frus...|        0|       1.0|\n",
      "|Wondering if anyo...|        0|       1.0|\n",
      "|Having not yet co...|        0|       1.0|\n",
      "+--------------------+---------+----------+\n",
      "\n",
      "+--------------------+---------+----------+\n",
      "|                text|Truthness|prediction|\n",
      "+--------------------+---------+----------+\n",
      "|Ime Udoka, head c...|        1|       1.0|\n",
      "|The Artemis I roc...|        1|       1.0|\n",
      "|Stefanos Tsitsipa...|        1|       1.0|\n",
      "|Arguing that the ...|        0|       1.0|\n",
      "|Addressing a rall...|        0|       1.0|\n",
      "|Citing a lack of ...|        0|       1.0|\n",
      "|California Gov. G...|        1|       1.0|\n",
      "|Pete Antonacci, t...|        1|       1.0|\n",
      "|An Arizona Superi...|        1|       1.0|\n",
      "|The Department of...|        1|       1.0|\n",
      "|Wyoming GOP Rep. ...|        1|       1.0|\n",
      "|Rihanna will head...|        1|       1.0|\n",
      "|A twelve-year-old...|        1|       1.0|\n",
      "|Clicking back and...|        0|       1.0|\n",
      "|Expressing shock ...|        0|       1.0|\n",
      "|Saying those unab...|        0|       1.0|\n",
      "|Assuring the nati...|        0|       1.0|\n",
      "|Muttering in frus...|        0|       1.0|\n",
      "|Wondering if anyo...|        0|       1.0|\n",
      "|Having not yet co...|        0|       1.0|\n",
      "+--------------------+---------+----------+\n",
      "\n",
      "+--------------------+---------+----------+\n",
      "|                text|Truthness|prediction|\n",
      "+--------------------+---------+----------+\n",
      "|Ime Udoka, head c...|        1|       1.0|\n",
      "|The Artemis I roc...|        1|       1.0|\n",
      "|Stefanos Tsitsipa...|        1|       0.0|\n",
      "|Arguing that the ...|        0|       0.0|\n",
      "|Addressing a rall...|        0|       0.0|\n",
      "|Citing a lack of ...|        0|       0.0|\n",
      "|California Gov. G...|        1|       0.0|\n",
      "|Pete Antonacci, t...|        1|       1.0|\n",
      "|An Arizona Superi...|        1|       1.0|\n",
      "|The Department of...|        1|       1.0|\n",
      "|Wyoming GOP Rep. ...|        1|       0.0|\n",
      "|Rihanna will head...|        1|       0.0|\n",
      "|A twelve-year-old...|        1|       1.0|\n",
      "|Clicking back and...|        0|       1.0|\n",
      "|Expressing shock ...|        0|       1.0|\n",
      "|Saying those unab...|        0|       1.0|\n",
      "|Assuring the nati...|        0|       1.0|\n",
      "|Muttering in frus...|        0|       0.0|\n",
      "|Wondering if anyo...|        0|       0.0|\n",
      "|Having not yet co...|        0|       0.0|\n",
      "+--------------------+---------+----------+\n",
      "\n",
      "+--------------------+---------+----------+\n",
      "|                text|Truthness|prediction|\n",
      "+--------------------+---------+----------+\n",
      "|Ime Udoka, head c...|        1|       1.0|\n",
      "|The Artemis I roc...|        1|       1.0|\n",
      "|Stefanos Tsitsipa...|        1|       0.0|\n",
      "|Arguing that the ...|        0|       0.0|\n",
      "|Addressing a rall...|        0|       1.0|\n",
      "|Citing a lack of ...|        0|       1.0|\n",
      "|California Gov. G...|        1|       1.0|\n",
      "|Pete Antonacci, t...|        1|       0.0|\n",
      "|An Arizona Superi...|        1|       1.0|\n",
      "|The Department of...|        1|       1.0|\n",
      "|Wyoming GOP Rep. ...|        1|       1.0|\n",
      "|Rihanna will head...|        1|       0.0|\n",
      "|A twelve-year-old...|        1|       1.0|\n",
      "|Clicking back and...|        0|       1.0|\n",
      "|Expressing shock ...|        0|       1.0|\n",
      "|Saying those unab...|        0|       1.0|\n",
      "|Assuring the nati...|        0|       1.0|\n",
      "|Muttering in frus...|        0|       0.0|\n",
      "|Wondering if anyo...|        0|       1.0|\n",
      "|Having not yet co...|        0|       1.0|\n",
      "+--------------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction2_lr_word2vec.select(['text','Truthness','prediction']).show()\n",
    "prediction2_gbt_word2vec.select(['text','Truthness','prediction']).show()\n",
    "prediction2_lr_hashtf.select(['text','Truthness','prediction']).show()\n",
    "prediction2_gbt_hashtf.select(['text','Truthness','prediction']).show()\n",
    "# this will show the entire tables, since there are only 20 articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f69db9-ea3b-4fdc-9dd7-5a49076ef6c6",
   "metadata": {},
   "source": [
    "So the word2vec algorithms (first two tables) predicted everything True in TestSet.csv.  Perhaps the Onion articles were a little too difficult given the satirical and 'real-sounding' nature of them.\n",
    "\n",
    "The HashingTF algorithms did make different predictions.  In the LR Hashing case (third table), it predicted 6/10 True articles correct and 6/10 Fake articles correct.\n",
    "\n",
    "In the GBT Hashing case (fourth table), it predicted 7/10 True articles correctly but only 2/10 Fake articles correctly.  At least this case appears to have a good true positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e923e2d9-851c-4e6b-8ad6-d7de7ccceff5",
   "metadata": {},
   "source": [
    "Evaluation of Neural Network Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bdb962-dec1-4060-8469-70cc1496da77",
   "metadata": {},
   "source": [
    "Accuracy on test subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46409bd4-9a12-4de8-9b50-d28c8dc75acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 4s 11ms/step - loss: 0.0436 - accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9883073568344116"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_numeric = tokenizer.texts_to_sequences(test_x)\n",
    "test_numeric_padded = pad_sequences(test_numeric,maxlen=maxlength)\n",
    "score = nnmodel.evaluate(test_numeric_padded,test_y)\n",
    "score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24f019-59a7-47a7-a600-953fe6a4462a",
   "metadata": {},
   "source": [
    "This is pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff5e7f-1d6f-4960-a8e3-ed2de7838b8a",
   "metadata": {},
   "source": [
    "Accuracy on TestSet.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20435000-1bee-4a46-b216-4d1edd13b4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step - loss: 2.3347 - accuracy: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.550000011920929"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_numeric = tokenizer.texts_to_sequences(TestSet['text'])\n",
    "testset_numeric_padded = pad_sequences(testset_numeric,maxlen=maxlength)\n",
    "score = nnmodel.evaluate(testset_numeric_padded,TestSet['Truthness'])\n",
    "score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418688d7-398b-425a-bb6a-62dc57249cee",
   "metadata": {},
   "source": [
    "This however, much like the ML models above with TestSet.csv, is not so good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08f8a875-6405-49c2-9126-483b26b42d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.9990616 ]\n",
      " [0.99815124]\n",
      " [0.08657873]\n",
      " [0.949527  ]\n",
      " [0.04332609]\n",
      " [0.9990277 ]\n",
      " [0.9991571 ]\n",
      " [0.99926466]\n",
      " [0.9979581 ]\n",
      " [0.9990293 ]\n",
      " [0.9967133 ]\n",
      " [0.00583255]\n",
      " [0.00227563]\n",
      " [0.9967611 ]\n",
      " [0.9948659 ]\n",
      " [0.03549943]\n",
      " [0.99750704]\n",
      " [0.05245351]\n",
      " [0.12646621]\n",
      " [0.9970225 ]]\n",
      "0     1\n",
      "1     1\n",
      "2     1\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     1\n",
      "7     1\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "11    1\n",
      "12    1\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "Name: Truthness, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(nnmodel.predict(testset_numeric_padded,verbose=1))\n",
    "print(TestSet['Truthness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607e3ca-47d3-4c9d-964a-2e0ae0236ef6",
   "metadata": {},
   "source": [
    "After carefully comparing the two tables, one can see that this model predicted 7/10 True articles correctly, but only 4/10 Fake articles correctly for TestSet.csv.  So it performed very similarly to the GBT Hashing model above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863afef-2f09-422e-b083-2267be66f866",
   "metadata": {},
   "source": [
    "In summary, True.csv consisting of Reuters articles only is likely biasing what the algorithms perceive as True.  In addition, many of the articles from Fake.csv appear to be op-ed style articles, and not necessarily articles with declaratively false elements (they are false in the sense that they are subjective and likely have a higher prevalence of adjectives).  These are likely easier to distinguish.  On the other hand, Onion articles from TestSet.csv, as satire, likely don't read as op-ed style articles.  They often make declaratively false statements and thus may be more difficult to distinguish from True articles which make declaratively true statements.\n",
    "\n",
    "So the models may be thinking: if news-style, then True; else if op-ed style, then Fake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603485bd-6001-417e-bc83-c66d2b82a73a",
   "metadata": {},
   "source": [
    "Returning back to the original big set of data, here we will build a barplot to see the breakdown of the confusion matrix elements for the 5 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e202b9bb-eb1a-4ab4-b9a6-413f1ac9fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 3s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# first we build the confusion matrices for each model using confusionMatrix() from pyspark.mllib\n",
    "lr_word2vec_matrix = MulticlassMetrics(prediction_lr_word2vec.select('Truthness','prediction').rdd.map(tuple)).confusionMatrix().toArray()\n",
    "gbt_word2vec_matrix = MulticlassMetrics(prediction_gbt_word2vec.select('Truthness','prediction').rdd.map(tuple)).confusionMatrix().toArray()\n",
    "lr_hashtf_matrix = MulticlassMetrics(prediction_lr_hashtf.select('Truthness','prediction').rdd.map(tuple)).confusionMatrix().toArray()\n",
    "gbt_hashtf_matrix = MulticlassMetrics(prediction_gbt_hashtf.select('Truthness','prediction').rdd.map(tuple)).confusionMatrix().toArray()\n",
    "# use confusion_matrix from sklearn for NN model since this one was \"in memory\",\n",
    "# note: the data in nnmodel.predict is probabilities, so we round it with np.around for the prediction\n",
    "lstm_matrix = confusion_matrix(np.around(nnmodel.predict(test_numeric_padded)), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c1504fd-b319-4e88-8caf-fc7355e8983b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True Positives</td>\n",
       "      <td>4487.0</td>\n",
       "      <td>LR-Word2Vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True Positives</td>\n",
       "      <td>4194.0</td>\n",
       "      <td>GBT-Word2Vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True Positives</td>\n",
       "      <td>4213.0</td>\n",
       "      <td>LR-HashTF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True Positives</td>\n",
       "      <td>4284.0</td>\n",
       "      <td>GBT-HashTF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True Positives</td>\n",
       "      <td>4641.0</td>\n",
       "      <td>LSTM NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False Positives</td>\n",
       "      <td>149.0</td>\n",
       "      <td>LR-Word2Vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False Positives</td>\n",
       "      <td>486.0</td>\n",
       "      <td>GBT-Word2Vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False Positives</td>\n",
       "      <td>119.0</td>\n",
       "      <td>LR-HashTF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False Positives</td>\n",
       "      <td>406.0</td>\n",
       "      <td>GBT-HashTF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False Positives</td>\n",
       "      <td>47.0</td>\n",
       "      <td>LSTM NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False Negatives</td>\n",
       "      <td>296.0</td>\n",
       "      <td>LR-Word2Vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False Negatives</td>\n",
       "      <td>589.0</td>\n",
       "      <td>GBT-Word2Vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False Negatives</td>\n",
       "      <td>570.0</td>\n",
       "      <td>LR-HashTF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False Negatives</td>\n",
       "      <td>499.0</td>\n",
       "      <td>GBT-HashTF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False Negatives</td>\n",
       "      <td>58.0</td>\n",
       "      <td>LSTM NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>True Negatives</td>\n",
       "      <td>4150.0</td>\n",
       "      <td>LR-Word2Vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>True Negatives</td>\n",
       "      <td>3813.0</td>\n",
       "      <td>GBT-Word2Vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>True Negatives</td>\n",
       "      <td>4180.0</td>\n",
       "      <td>LR-HashTF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>True Negatives</td>\n",
       "      <td>3893.0</td>\n",
       "      <td>GBT-HashTF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>True Negatives</td>\n",
       "      <td>4234.0</td>\n",
       "      <td>LSTM NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Category  Frequency         Model\n",
       "0    True Positives     4487.0   LR-Word2Vec\n",
       "1    True Positives     4194.0  GBT-Word2Vec\n",
       "2    True Positives     4213.0     LR-HashTF\n",
       "3    True Positives     4284.0    GBT-HashTF\n",
       "4    True Positives     4641.0       LSTM NN\n",
       "5   False Positives      149.0   LR-Word2Vec\n",
       "6   False Positives      486.0  GBT-Word2Vec\n",
       "7   False Positives      119.0     LR-HashTF\n",
       "8   False Positives      406.0    GBT-HashTF\n",
       "9   False Positives       47.0       LSTM NN\n",
       "10  False Negatives      296.0   LR-Word2Vec\n",
       "11  False Negatives      589.0  GBT-Word2Vec\n",
       "12  False Negatives      570.0     LR-HashTF\n",
       "13  False Negatives      499.0    GBT-HashTF\n",
       "14  False Negatives       58.0       LSTM NN\n",
       "15   True Negatives     4150.0   LR-Word2Vec\n",
       "16   True Negatives     3813.0  GBT-Word2Vec\n",
       "17   True Negatives     4180.0     LR-HashTF\n",
       "18   True Negatives     3893.0    GBT-HashTF\n",
       "19   True Negatives     4234.0       LSTM NN"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a dataframe with which to plot confusion matrix frequencies\n",
    "# 20x3 dataframe with columns 'Category', 'Frequency', and 'Model'\n",
    "matrices = [lr_word2vec_matrix, gbt_word2vec_matrix, lr_hashtf_matrix, gbt_hashtf_matrix, lstm_matrix]\n",
    "categories = 5*['True Positives']+5*['False Positives']+5*['False Negatives']+5*['True Negatives']\n",
    "matrix_names = 4*['LR-Word2Vec','GBT-Word2Vec','LR-HashTF','GBT-HashTF','LSTM NN']\n",
    "TP = [matrix[0,0] for matrix in matrices]\n",
    "FP = [matrix[0,1] for matrix in matrices]\n",
    "FN = [matrix[1,0] for matrix in matrices]\n",
    "TN = [matrix[1,1] for matrix in matrices]\n",
    "cm_dict = {'Category':categories, 'Frequency':TP+FP+FN+TN, 'Model':matrix_names}\n",
    "DF_CM = pd.DataFrame(cm_dict)\n",
    "DF_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14a3d4f9-d129-4d90-a3b3-a2b385ba5f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Category', ylabel='Frequency'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x='Category',y='Frequency',hue='Model',data=DF_CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73562450-a299-4b03-8cbf-fb65a34eb21a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
